{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks (ANN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "* [Background](#Background)\n",
    "    \n",
    "* [How it Works](#How-it-Works)\n",
    "    \n",
    "* [Algorithm](#Algorithm)\n",
    "\n",
    "* [Tips](#Tips)\n",
    "\n",
    "* [Example: MNIST Dataset](#Example:-MNIST-Dataset)\n",
    "\n",
    "    * [Custom Neural Network](#Custom-Neural-Network)\n",
    "    \n",
    "    * [Keras Neural Network](#Keras-Neural-Network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "The latest resurgence of research in artificial intelligence and machine learning can all be attributed to the latest advancements in the backpropogation algorithm and the Artificial Neural Network (ANN) model. This model was originally invented by Warren McCulloch and Walter Pitts in 1943, which began to stagnate in 1969, until interest was renewed in 1975 when Paul Werbos introduced his backpropogation algorithm. The main causes of the stagnation were due to the limitations of neural networks, pointed out by Marvin Minsky and Seymour Papert, which included the inability of the perceptron, the basic unit of a neural network, to calculate the exclusive-or circuit and the fact that computers didn't have enough processing power to effectively handle the work required by large neural networks. The backpropogation algorithm solved both of these problems by making the training of multi-layer neural networks both possible and efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How it Works\n",
    "\n",
    "The architecture of a neural network is modeled on the structure of neurons in the human brain; taking input signals and propogating those signals to other neurons, in turn, performing some sort of calculation in the process which results in an output. The neural network model takes the inner workings of the biological neuron and, argueably,  simplifies it down to an input-output function. There are several key characteristics that define a neural network; those include: an activation function, a learning rate, backpropogation, a cost function, and a mechanism for updating weights. Let's consider a simple ANN structure consisting of 3 input nodes, a single hidden layer with 4 nodes, and an output layer with 2 nodes as shown below.\n",
    "\n",
    "<img src = "img_1.png" width = "450" /> ![](img_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an ANN, you have an **input layer**, an **output layer**, and an arbitrary number of **hidden layers**, the layers that lie in between the input and the output. In each layer lie perceptrons which are called the **nodes**. Each node of each layer is connected to all other nodes in both the preceeding and proceeding layers, unless they are input or output layers in which case only one of the two apply. Now, quantitative input signal(s), training observations, enter the ANN at the input layer. Each signal is then propogated forward to the next layer but not before being applied a **weight**. Each signal, represented as a black arrow in the figure above, possesses its own weight which is, initially, assigned randomly with a value between zero and one. These weights are the source of an ANN's ability to calculate the output using its input. The signal that a node obtains in the following layer is the sum of the products of the signal and their corresponding weights. \n",
    "\n",
    "Before the signals continue on to the following layers, the signals recieved from the input nodes are passed through an **activation function**, commonly the sigmoid function, which is used to simulate neurons 'failing' to fire a signal until they reach some threshold level of signals from other neurons.\n",
    "\n",
    "<img src = "img_2.png" width = "450" /> ![](img_2.png)",
    "\n",
    "The signals continue this process of being applied weights and passing through activation functions until it reaches the output layer and the final output is produced. The resulting output of the ANN is then compared to the true value of the output. The difference between the two, the error, is determined using a **cost function**. There are various cost functions one may use including mean squared error, maximum likelihood, and sum squared error. The function one uses is dependent on the goal of the analyst and the interpretation of the output.\n",
    "\n",
    "The next step is to use the error to update the weights throughout the model. The error is propogated backwards into the model and split amongst the nodes with more of the error going to those contributing connections which had greater link weights as they contributed more to the error; this is called **backpropogation**. The proportion of the error given to a node is defined by (node weight/ sum of signal weights).\n",
    "\n",
    "To actually update the weights, we utilize **gradient descent** which is an optimization algorithm that determines how the error rate changes as the weight of some link, (w)ij changes; the slope of the gradient. The goal is to minimize this slope as much as possible. This slope is calculated as follows:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial E}{\\partial w_{ij}} = -(e_j) * sigmoid\\left( \\sum_{i} w_{ij} o_i \\right) * (1 - sigmoid\\left( \\sum_{i} w_{ij} o_i \\right)) * o_i\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial E}{\\partial w_{jk}} = -(t_k - o_k) * sigmoid\\left( \\sum_{j} w_{jk} o_j \\right) * (1 - sigmoid\\left( \\sum_{j} w_{jk} o_j \\right)) * o_j\n",
    "\\end{equation*}\n",
    "\n",
    "[Here](https://youtu.be/5u0jaA3qAGk?t=240) is a clip of a video by Welch Labs that illustrates the process of gradient descent.\n",
    "\n",
    "The first formula applies to weights between the input and first hidden layer while the second applies to the weights between hidden layers and between the last hidden layer and the output layer. The first part is simply the (target - actual) error. The sum expression inside the sigmoids is the signal into the final layer node; it’s the signal into a node before the activation function is applied. The last part, oj, is the output from the previous hidden layer node j. For any weight, the result of this function is used to assign a new weight by the following:\n",
    "\n",
    "\\begin{equation*}\n",
    "new(w_{jk}) = old(w_{jk}) - \\alpha\\ * \\frac{\\partial E}{\\partial w_{jk}}\n",
    "\\end{equation*}\n",
    "\n",
    "When the weights are updated after each input observation is evaluated, it is called *Stochastic Gradient Descent*. If the weights are updated only after all of the observations have been evaluated, it is called *Batch Gradient Descent*. The former is the standard used as it prevents the model from getting stuck in local minimas. Alpha represents the **learning rate**. The link weights in the model need to be updated in such a way that changes to the link weights are not overwhelmed by any single training example. The learning rate is a value that scales the suggested change to the weight so that it is not over-exaggerated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The machine learning community has iterated feverishly on the classic ANN structure creating numerous packages allowing one to use neural nets straight out of the box. Although, it is important for a data scientist to be able to build his/her own neural net from scratch in order to solidify their understanding of the model. In addition, building a model from scratch allows one to improve on its structure to increase efficiency in numerous ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple, 3-layer ANN\n",
    "\n",
    "class neuralNetwork:\n",
    "\n",
    "    # initialise the neural network\n",
    "    def __init__(self, inputnodes, hiddennodes, outputnodes, learningrate):\n",
    "        self.inodes = inputnodes # set number of nodes in each input, hidden, output layer\n",
    "        self.hnodes = hiddennodes\n",
    "        self.onodes = outputnodes\n",
    "        self.wih = np.random.normal(0.0, pow(self.hnodes, -0.5), (self.hnodes, self.inodes))# link weight matrices, wih and who\n",
    "        self.who = np.random.normal(0.0, pow(self.onodes, -0.5), (self.onodes, self.hnodes))# weights inside the arrays are w_i_j, where link is from node i to node j in the next layer\n",
    "        self.lr = learningrate # learning rate\n",
    "        self.activation_function = lambda x: sps.expit(x) # activation function is the sigmoid function\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    # train the neural network\n",
    "    def train(self, inputs_list, targets_list):\n",
    "        # convert inputs list to 2d array\n",
    "        inputs = np.array(inputs_list, ndmin=2).T\n",
    "        targets = np.array(targets_list, ndmin=2).T\n",
    "\n",
    "        # calculate signals into hidden layer\n",
    "        hidden_inputs = np.dot(self.wih, inputs)\n",
    "        # calculate the signals emerging from hidden layer\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "\n",
    "        # calculate signals into final output layer\n",
    "        final_inputs = np.dot(self.who, hidden_outputs)\n",
    "        # calculate the signals emerging from final output layer\n",
    "        final_outputs = self.activation_function(final_inputs)\n",
    "\n",
    "        # output layer error is the (target - actual)\n",
    "        output_errors = targets - final_outputs\n",
    "        # hidden layer error is the output_errors, split by weights, recombined at hidden nodes\n",
    "        hidden_errors = np.dot(self.who.T, output_errors)\n",
    "\n",
    "        # update the weights for the links between the hidden and output layers\n",
    "        self.who += self.lr * np.dot((output_errors * final_outputs * (1.0 - final_outputs)), np.transpose(hidden_outputs))\n",
    "\n",
    "        # update the weights for the links between the input and hidden layers\n",
    "        self.wih += self.lr * np.dot((hidden_errors * hidden_outputs * (1.0 - hidden_outputs)), np.transpose(inputs))\n",
    "\n",
    "        pass\n",
    "    \n",
    "    # query the neural network\n",
    "    def query(self, inputs_list):\n",
    "        # convert inputs list to 2d array\n",
    "        inputs = np.array(inputs_list, ndmin=2).T\n",
    "\n",
    "        # calculate signals into hidden layer\n",
    "        hidden_inputs = np.dot(self.wih, inputs)\n",
    "        # calculate the signals emerging from hidden layer\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "\n",
    "        # calculate signals into final output layer\n",
    "        final_inputs = np.dot(self.who, hidden_outputs)\n",
    "        # calculate the signals emerging from final output layer\n",
    "        final_outputs = self.activation_function(final_inputs)\n",
    "\n",
    "        return final_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Rescale inputs into the range of 0 to 1, or 0.01 to 0.99. Some will add a small offset to the inputs, like 0.01, just to avoid having zero inputs which are troublesome because they kill the learning ability by zeroing the weight update expression by setting that oj = 0.\n",
    "\n",
    "\n",
    "* Avoid large initial weights as they reduce the model's ability to learn better weights. Initializing weights randomly and uniformly from a range between -1.0 to +1.0 will suffice. Rules of thumb are available for specific network shapes and specific activation functions. Makes sure each weight is different and that no weight is zero.\n",
    "\n",
    "\n",
    "* There are several ways you can improve your ANN:\n",
    "    * Adjust the learning rate.\n",
    "    * Repeat the training several times against the dataset; tweaking the number of 'epochs'.\n",
    "    * Change the number of hidden nodes/layers.\n",
    "    * Try a different activation function. (i.e. Threshold Function, Rectifier Function, Hyperbolic Tangent/tanh)\n",
    "    * Try a different cost function.\n",
    "    * Change the type of input data you feed into the network.\n",
    "    \n",
    "    \n",
    "* Run experiments and plot your data so that you can visualize how the performance of your ANN changes as you tweak one parameter or another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create an artificial neural network and use the MNIST dataset, created by Yann LeCun, which consists of images of handwritten digits and the grey levels of each pixel in the image. The data can be found [here](http://yann.lecun.com/exdb/mnist/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.special as sps\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('mnist_train.csv')\n",
    "\n",
    "test = pd.read_csv('mnist_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>1x1</th>\n",
       "      <th>1x2</th>\n",
       "      <th>1x3</th>\n",
       "      <th>1x4</th>\n",
       "      <th>1x5</th>\n",
       "      <th>1x6</th>\n",
       "      <th>1x7</th>\n",
       "      <th>1x8</th>\n",
       "      <th>1x9</th>\n",
       "      <th>...</th>\n",
       "      <th>28x19</th>\n",
       "      <th>28x20</th>\n",
       "      <th>28x21</th>\n",
       "      <th>28x22</th>\n",
       "      <th>28x23</th>\n",
       "      <th>28x24</th>\n",
       "      <th>28x25</th>\n",
       "      <th>28x26</th>\n",
       "      <th>28x27</th>\n",
       "      <th>28x28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  1x1  1x2  1x3  1x4  1x5  1x6  1x7  1x8  1x9  ...    28x19  28x20  \\\n",
       "0      5    0    0    0    0    0    0    0    0    0  ...        0      0   \n",
       "1      0    0    0    0    0    0    0    0    0    0  ...        0      0   \n",
       "2      4    0    0    0    0    0    0    0    0    0  ...        0      0   \n",
       "3      1    0    0    0    0    0    0    0    0    0  ...        0      0   \n",
       "4      9    0    0    0    0    0    0    0    0    0  ...        0      0   \n",
       "\n",
       "   28x21  28x22  28x23  28x24  28x25  28x26  28x27  28x28  \n",
       "0      0      0      0      0      0      0      0      0  \n",
       "1      0      0      0      0      0      0      0      0  \n",
       "2      0      0      0      0      0      0      0      0  \n",
       "3      0      0      0      0      0      0      0      0  \n",
       "4      0      0      0      0      0      0      0      0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 785)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>1x1</th>\n",
       "      <th>1x2</th>\n",
       "      <th>1x3</th>\n",
       "      <th>1x4</th>\n",
       "      <th>1x5</th>\n",
       "      <th>1x6</th>\n",
       "      <th>1x7</th>\n",
       "      <th>1x8</th>\n",
       "      <th>1x9</th>\n",
       "      <th>...</th>\n",
       "      <th>28x19</th>\n",
       "      <th>28x20</th>\n",
       "      <th>28x21</th>\n",
       "      <th>28x22</th>\n",
       "      <th>28x23</th>\n",
       "      <th>28x24</th>\n",
       "      <th>28x25</th>\n",
       "      <th>28x26</th>\n",
       "      <th>28x27</th>\n",
       "      <th>28x28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  1x1  1x2  1x3  1x4  1x5  1x6  1x7  1x8  1x9  ...    28x19  28x20  \\\n",
       "0      7    0    0    0    0    0    0    0    0    0  ...        0      0   \n",
       "1      2    0    0    0    0    0    0    0    0    0  ...        0      0   \n",
       "2      1    0    0    0    0    0    0    0    0    0  ...        0      0   \n",
       "3      0    0    0    0    0    0    0    0    0    0  ...        0      0   \n",
       "4      4    0    0    0    0    0    0    0    0    0  ...        0      0   \n",
       "\n",
       "   28x21  28x22  28x23  28x24  28x25  28x26  28x27  28x28  \n",
       "0      0      0      0      0      0      0      0      0  \n",
       "1      0      0      0      0      0      0      0      0  \n",
       "2      0      0      0      0      0      0      0      0  \n",
       "3      0      0      0      0      0      0      0      0  \n",
       "4      0      0      0      0      0      0      0      0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 785)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(train['label'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's scale the data so that all values are between 0.01 - 1.00 instead of 0-255; we'll need to divide by 255 then \n",
    "# multiply by 0.99 and then add 0.01 to each value.\n",
    "\n",
    "train_new = train.iloc[:,1:].applymap(lambda x: ((x/255)*0.99) + 0.01)\n",
    "\n",
    "train_new['label'] = train['label']\n",
    "\n",
    "train_new = np.array(train_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_new = test.iloc[:,1:].applymap(lambda x: ((x/255)*0.99) + 0.01)\n",
    "\n",
    "test_new['label'] = test['label']\n",
    "\n",
    "test_new = np.array(test_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ANN, in this case, will act as a classifier labeling each observation with the number it thinks it 'sees' between\n",
    "0 and 9. So, we can have 10 neurons in the output layer, one for each number 0-9, and, in theory, the numbered neuron that matches the number in the image should be the only one to fire, all others being silent.\n",
    "\n",
    "The target label for any single observation will be 0.99 at the correct neuron and 0.01 averywhere else. This can be \n",
    "represented as a vector "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of input, hidden and output nodes\n",
    "input_nodes = 784\n",
    "hidden_nodes = 200\n",
    "output_nodes = 10\n",
    "\n",
    "# learning rate is 0.1\n",
    "learning_rate = 0.1\n",
    "\n",
    "# create instance of neural network\n",
    "n = neuralNetwork(input_nodes,hidden_nodes,output_nodes, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for obs in train_new:\n",
    "    targets = np.zeros(output_nodes) + 0.01\n",
    "    targets[int(obs[-1])] = 0.99\n",
    "    n.train(obs[0:-1],targets)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the neural network\n",
    "\n",
    "# scorecard for how well the network performs, initially empty\n",
    "scorecard = []\n",
    "\n",
    "# go through all the records in the test data set\n",
    "for record in test_new:\n",
    "    \n",
    "    # correct answer is first value\n",
    "    correct_label = int(record[-1])\n",
    "    #print(correct_label, \" is the correct label\")\n",
    "\n",
    "    # query the network\n",
    "    outputs = n.query(record[0:-1])\n",
    "\n",
    "    # the index of the highest value corresponds to the label\n",
    "    label = np.argmax(outputs)\n",
    "    #print(label, \" is the network's answer\")\n",
    "\n",
    "    # append correct or incorrect to list\n",
    "    if (label == correct_label):\n",
    "        # network's answer matches correct answer, add 1 to scorecard\n",
    "        scorecard.append(1)\n",
    "    else:\n",
    "        # network's answer doesn't match correct answer, add 0 to scorecard\n",
    "        scorecard.append(0)\n",
    "        pass\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent Accuracy:  95.7 %\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print('Percent Accuracy: ', round(((Counter(scorecard)[1])/10000)*100,2), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we see that our ANN has become a classifier with a test accuracy of ~96%. We can be sure that tweaking the numerous parameters in the model would be able to increase the accuracy even further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When not producing a state-of-the-art model, there are many packages and environments that one can take advantage of when creating an artificial neural network. Such well known packages include 'Keras', 'TensorFlow', and 'Theano'. Theano is an open source compuation library that makes fast, efficient computation on the computer's GPU. TensorFlow is a package, similar to Theano, created by Google. Keras is an open source neural network library written in Python capable of running on top of TensorFlow and Theano. Here, we will use Keras to build an ANN similar to our simple, previous 3-layer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreating the input data\n",
    "\n",
    "train_new = train.iloc[:,1:].applymap(lambda x: ((x/255)*0.99) + 0.01)\n",
    "\n",
    "train_new['label'] = train['label']\n",
    "\n",
    "x_train = np.array(train_new.iloc[:,0:-1])\n",
    "\n",
    "y_train = []\n",
    "for obs in train_new.iloc[:,-1]:\n",
    "    target = list(np.zeros(10) + 0.01)\n",
    "    target[int(obs)] = 0.99\n",
    "    y_train.append(target)\n",
    "\n",
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_new = test.iloc[:,1:].applymap(lambda x: ((x/255)*0.99) + 0.01)\n",
    "\n",
    "test_new['label'] = test['label']\n",
    "\n",
    "x_test = np.array(test_new.iloc[:,0:-1])\n",
    "y_test = np.array(test_new.iloc[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 190s 3ms/step - loss: 0.7015 - acc: 0.9419\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a67466550>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initializing the ANN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(units = 200, kernel_initializer = 'uniform', activation = 'sigmoid', input_dim = 784))\n",
    "\n",
    "# If adding another hidden layer\n",
    "# classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
    "\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(units = 10, kernel_initializer = 'uniform', activation = 'softmax'))\n",
    "\n",
    "# Compiling the ANN\n",
    "classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Fitting the ANN to the Training set\n",
    "classifier.fit(x_train, y_train, batch_size = 1, epochs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the test set results\n",
    "\n",
    "y_pred = classifier.predict(x_test)\n",
    "y_pred = [np.argmax(item) for item in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scorecard for how well the network performs, initially empty\n",
    "scorecard = []\n",
    "\n",
    "# go through all the records in the test data set\n",
    "for i in range(0,len(y_pred)):\n",
    "\n",
    "    # append correct or incorrect to list\n",
    "    if (y_pred[i] == y_test[i]):\n",
    "        # network's answer matches correct answer, add 1 to scorecard\n",
    "        scorecard.append(1)\n",
    "    else:\n",
    "        # network's answer doesn't match correct answer, add 0 to scorecard\n",
    "        scorecard.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent Accuracy:  96.85 %\n"
     ]
    }
   ],
   "source": [
    "print('Percent Accuracy: ', round(((Counter(scorecard)[1])/10000)*100,2), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have used the Keras package to create a neural network model, identical to the one built from scratch, that has an  improved accuracy of ~97% and was built with fewer lines of code than were necessary to process the input data. This just goes to show the power that packages such as Keras provide to the common user. With such packages, one has the ability to easily build more complex and deeper neural networks. Deep learning constitutes the work done by an artificial neural network that possesses multiple hidden layers in its structure. Standardized packages make the creation of such complex models easier than ever before."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
